{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFKTnsMQXImbhLs4/UPplS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Annie00000/Project/blob/main/1_11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Logger"
      ],
      "metadata": {
        "id": "o6gl4h5GCscg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def daily_logger():\n",
        "  local_time = datetime.datetime.now()\n",
        "  pid = os.getpid()\n",
        "  with open('../../../../../sever_ip','r') as file:\n",
        "    server_IP = file.readline().strip()\n",
        "  #server_IP = socket.gethostbyname(socket.gethostname()) # 獲取當前主機的IP地址\n",
        "  date = local_time.strftime('%Y-%m-%d')\n",
        "  if 'log' not in os.listdir('./'):\n",
        "    os.mkdir('./log')\n",
        "  logger = logging.getLogger(f'{date}__log') # 创建或获取一个名为 '{date}__log' 的日志记录器（logger）对象，并将其赋值给变量 logger\n",
        "  if not logger.handlers:\n",
        "    handler = logging.FileHandler(f'./log/{date}.txt')\n",
        "    handler.setLevel(logging.INFO)\n",
        "    formatter = logging.Formater(f'[%(asctime)s__{server_IP}__{pid}]:%(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
        "    handler.setFormatter(formatter) # 将上面创建的格式化器应用到文件处理器(handler)上。 !!!!! 增加這行!!!!!\n",
        "    logger.addHandler(handler)\n",
        "  return logger"
      ],
      "metadata": {
        "id": "9crCw-HdCutT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## train model"
      ],
      "metadata": {
        "id": "DqphL4LACver"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1t1SufeY8Ha1"
      },
      "outputs": [],
      "source": [
        "def load_data(data_dir,search='*.png'):\n",
        "  image_paths = []\n",
        "  labels = []\n",
        "  for class_folder_name in os.listdir(data_dir):\n",
        "    class_folder_path = os.path.join(data_dir, class_folder_name)\n",
        "    for fpath in glob.glob(os.path.join(class_folder_path, search)):\n",
        "      image_paths.append(fpath)\n",
        "      labels.append(class_folder_name)\n",
        "  return image_paths, labels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataGenerator(Sequence):\n",
        "    def __init__(self, image_paths, labels, batch_size=16, target_size=(224,224), label_to_index, num_classes, shuffle=True):\n",
        "        self.image_paths = np.array(image_paths)\n",
        "        self.labels = np.array(labels)\n",
        "        self.batch_size = batch_size\n",
        "        self.target_size = target_size\n",
        "        #self.augmentation_dict = augmentation_dict\n",
        "        self.label_to_index = label_to_index\n",
        "        self.num_classes = num_classes\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.image_paths) / float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_images = []\n",
        "        batch_labels = []\n",
        "\n",
        "        for i in batch_indices:\n",
        "            img_path = self.image_paths[i]\n",
        "            label = self.labels[i]\n",
        "\n",
        "            # 加载和预处理图像 (直接在 TensorFlow 中載入和解碼映像。這意味著從一開始圖像就是以 Tensor 的形式存在的)\n",
        "            img = tf.io.read_file(img_path)\n",
        "            img = tf.image.decode_image(img, channels=3)\n",
        "            img = tf.image.resize(img, self.target_size)\n",
        "\n",
        "            # 应用数据增强\n",
        "            img = self.apply_augmentation(img, label)/ 255 # 規一化\n",
        "            batch_images.append(img)\n",
        "            batch_labels.append(self.label_to_index[label])\n",
        "\n",
        "        return tf.convert_to_tensor(batch_images), to_categorical(batch_labels, num_classes=self.num_classes)\n",
        "\n",
        "    def apply_augmentation(self, image):\n",
        "        # 随机旋转（正负 10 度）\n",
        "        rotation_degree = random.uniform(-10, 10) #np.random.uniform(-10, 10)\n",
        "        image = rotate(image, rotation_degree, reshape=False, mode='nearest')\n",
        "          # reshape=False 保证旋转后的图像大小不变，但这可能导致图像的一部分被裁剪\n",
        "          # mode 决定了在旋转过程中如何处理图像边界之外的像素\n",
        "\n",
        "        # 对比度增强\n",
        "        contrast_factor = 1.5  # 可以根据需要调整这个值\n",
        "        image = self.adjust_contrast(image, contrast_factor)\n",
        "\n",
        "        return image\n",
        "\n",
        "    def adjust_contrast(self, image, contrast_factor):\n",
        "        \"\"\"调整图像的对比度\"\"\"\n",
        "        mean = np.mean(image, axis=(0, 1), keepdims=True)\n",
        "        adjusted = (image - mean) * contrast_factor + mean\n",
        "        return np.clip(adjusted, 0, 255)\n",
        "\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.indices = np.arange(len(self.image_paths))\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indices)\n",
        "\n"
      ],
      "metadata": {
        "id": "1ukVaakyA6j4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrained_model(data_dir, model_path):\n",
        "  ogger = daily_logger()\n",
        "  # 检查 CUDA 是否可用\n",
        "  cuda_available = tf.test.is_gpu_available(cuda_only=True)\n",
        "  if cuda_available:\n",
        "      logger.info(\"CUDA is available. Using GPU for training.\")\n",
        "  else:\n",
        "      logger.info(\"CUDA is not available. Using CPU for training.\")\n",
        "  image_paths, labels = load_data(data_dir)\n",
        "  test_image_paths, test_labels = load_data(data_dir,reaserch='*/*.png')\n",
        "  label_to_index = {label: index for index, label in enumerate(sorted(list(set(labels))))}\n",
        "  train_paths, val_paths, train_labels, val_labels = train_test_split(image_paths, labels, test_size=0.2, stratify=labels ,shuffle=True, random_state=42)\n",
        "\n",
        "  train_generator = CustomDataGenerator(\n",
        "    train_paths, train_labels, label_to_index=label_to_index, num_classes=len(label_to_index), shuffle=True)\n",
        "  val_generator = CustomDataGenerator(\n",
        "    val_paths, val_labels,   label_to_index=label_to_index, label_to_index=label_to_index, num_classes=len(label_to_index), shuffle=False)\n",
        "  test_generator = CustomDataGenerator(\n",
        "    test_paths, test_labels,  label_to_index=label_to_index, label_to_index=label_to_index, num_classes=len(label_to_index), shuffle=False)\n",
        "\n",
        "  model_ver = datetime.datetime.now().strftime('%Y-%m-%d_%H:%M')\n",
        "  model_name = 'Annie_'+model_ver\n",
        "\n",
        "  # callbacks\n",
        "  early_stopping = EarlyStopping(monitor='val_loss', patience=4, verbose=1)\n",
        "  model_checkpoint = ModelCheckpoint('model_best.h5', save_best_only=True, monitor='val_loss', mode='min') # 可改成.keras格式('model_best.keras')\n",
        "  reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.001, verbose=1)\n",
        "\n",
        "  callbacks = [early_stopping, model_checkpoint , reduce_lr]\n",
        "\n",
        "\n",
        "  model = load_model(model_path) #model_path輸入h5檔\n",
        "  # 记录训练开始时间\n",
        "  start_time = time.time()\n",
        "  history = model.fit(train_generator,\n",
        "        steps_per_epoch=len(train_paths) // 16,\n",
        "        validation_data=val_generator,\n",
        "        validation_steps=len(val_paths) // 16,\n",
        "        epochs=50,\n",
        "        callbacks=callbacks)\n",
        "  # 记录训练结束时间并计算训练总时长\n",
        "  end_time = time.time()\n",
        "  training_duration = end_time - start_time\n",
        "\n",
        "  # 记录训练时间、模型版本和测试准确率\n",
        "  logger.info(f\"Model training completed. Total time: {training_duration:.2f} seconds.\")\n",
        "  logger.info(f\"Model version: {model_name}. Test accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "  test_loss, test_accuracy = model.evaluate(test_generator)\n",
        "\n",
        "  return model_name, test_accuracy\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IRn2x2Lf9IZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 使用類class屬性"
      ],
      "metadata": {
        "id": "g8yIn0nmKIa0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelTrainer:\n",
        "    def __init__(self):\n",
        "        self.history = None\n",
        "\n",
        "    def retrained_model(self, data_dir, model_path):\n",
        "        # ... 训练过程 ...\n",
        "        self.history = model.fit(...)\n",
        "        # ... 其他代码 ...\n",
        "\n",
        "    def plot_training_history(self):\n",
        "        if self.history is not None:\n",
        "            plt.figure(figsize=(12, 5))\n",
        "            # ... 绘图代码 ...\n",
        "            plt.show()\n",
        "\n",
        "# 使用示例\n",
        "trainer = ModelTrainer()\n",
        "trainer.retrained_model('data/dir', 'model/path')\n",
        "trainer.plot_training_history()"
      ],
      "metadata": {
        "id": "IRrMEbxiLtG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageModelTrainer:\n",
        "    def __init__(self, data_dir, model_path):\n",
        "        self.data_dir = data_dir\n",
        "        self.model_path = model_path\n",
        "        self.logger = self._daily_logger()\n",
        "\n",
        "    def load_data(self, search='*.png'):\n",
        "        # ... 加载数据代码 ...\n",
        "\n",
        "    def retrained_model(self):\n",
        "        start_time = time.time()\n",
        "        self.logger.info(\"Model retraining started.\")\n",
        "\n",
        "        # ... 加载数据和准备生成器 ...\n",
        "\n",
        "        model = load_model(self.model_path)\n",
        "        history = model.fit(train_generator, validation_data=val_generator, epochs=50, callbacks=self._get_callbacks())\n",
        "\n",
        "        training_duration = time.time() - start_time\n",
        "        self.logger.info(f\"Model retraining completed. Duration: {training_duration:.2f} seconds.\")\n",
        "        return history\n",
        "\n",
        "    def plot_training_history(self, history):\n",
        "        start_time = time.time()\n",
        "        self.logger.info(\"Model retraining started.\")\n",
        "\n",
        "        # 加载数据和准备生成器\n",
        "        image_paths, labels = self.load_data()\n",
        "        label_to_index = {label: index for index, label in enumerate(sorted(set(labels)))}\n",
        "        # 将 label_to_index 字典转换为 DataFrame\n",
        "        label_to_index_df = pd.DataFrame(list(label_to_index.items()), columns=['Class_Name', 'Index'])\n",
        "        # 保存为 CSV 文件\n",
        "        label_to_index_csv_path = 'path/to/save/label_to_index.csv'\n",
        "        label_to_index_df.to_csv(label_to_index_csv_path, index=False)\n",
        "\n",
        "        self.logger.info(f\"Label to index mapping saved to {label_to_index_csv_path}\")\n",
        "\n",
        "\n",
        "\n",
        "        train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
        "            image_paths, labels, test_size=0.2, stratify=labels, shuffle=True, random_state=42\n",
        "        )\n",
        "\n",
        "        train_generator = CustomDataGenerator(\n",
        "            train_paths, train_labels, batch_size=16, target_size=(224, 224),\n",
        "            label_to_index=label_to_index, num_classes=len(label_to_index), shuffle=True\n",
        "        )\n",
        "        val_generator = CustomDataGenerator(\n",
        "            val_paths, val_labels, batch_size=16, target_size=(224, 224),\n",
        "            label_to_index=label_to_index, num_classes=len(label_to_index), shuffle=False\n",
        "        )\n",
        "\n",
        "        # 加载模型\n",
        "        model = load_model(self.model_path)\n",
        "\n",
        "        # 获取回调函数\n",
        "        callbacks = self._get_callbacks()\n",
        "\n",
        "        # 训练模型\n",
        "        history = model.fit(\n",
        "            train_generator,\n",
        "            validation_data=val_generator,\n",
        "            epochs=50,\n",
        "            callbacks=callbacks\n",
        "        )\n",
        "\n",
        "        # 获取模型版本\n",
        "        model_ver = datetime.datetime.now().strftime('%Y-%m-%d_%H:%M')\n",
        "        model_name = f\"Annie_{model_ver}\"\n",
        "\n",
        "        # 保存模型名到文本文件 (每天生成一個新的)\n",
        "        model_name_file = f\"{model_name}.txt\"\n",
        "        with open(model_name_file, 'w') as file:\n",
        "            file.write(model_name)\n",
        "\n",
        "        # 评估测试集\n",
        "        test_generator = CustomDataGenerator(\n",
        "            test_paths, test_labels, batch_size=16, target_size=(224, 224),\n",
        "            label_to_index=label_to_index, num_classes=len(label_to_index), shuffle=False\n",
        "        )\n",
        "        test_loss, test_accuracy = model.evaluate(test_generator)\n",
        "        self.logger.info(f\"Test accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "        training_duration = time.time() - start_time\n",
        "        self.logger.info(f\"Model retraining completed. Duration: {training_duration:.2f} seconds. Model version: {model_ver}\")\n",
        "\n",
        "        return model_ver, test_accuracy.\n",
        "\n",
        "    def _daily_logger(self):\n",
        "        logger = logging.getLogger(__name__)\n",
        "        logger.setLevel(logging.INFO)\n",
        "\n",
        "        if not logger.handlers:\n",
        "            handler = logging.FileHandler(filename=f\"./log/{datetime.datetime.now().strftime('%Y-%m-%d')}.log\")\n",
        "            formatter = logging.Formatter('[%(asctime)s]: %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
        "            handler.setFormatter(formatter)\n",
        "            logger.addHandler(handler)\n",
        "\n",
        "        return logger\n",
        "\n",
        "    def _get_callbacks(self):\n",
        "        model_ver = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        model_name = f\"model_{model_ver}.h5\"\n",
        "\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=4, verbose=1)\n",
        "        model_checkpoint = ModelCheckpoint(os.path.join(self.model_path, model_name), save_best_only=True, monitor='val_loss', mode='min')\n",
        "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.001, verbose=1)\n",
        "\n",
        "        return [early_stopping, model_checkpoint, reduce_lr]"
      ],
      "metadata": {
        "id": "XwlKC2KuVk5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## predict\n"
      ],
      "metadata": {
        "id": "mdDofgNeWVyO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "import numpy as np\n",
        "import logging\n",
        "import json\n",
        "\n",
        "def predict_image_class(model_path, image_path, output_file, target_size=(224, 224)):\n",
        "    # 获取日志记录器\n",
        "    logger = daily_logger()\n",
        "\n",
        "    try:\n",
        "        # 记录正在加载的模型\n",
        "        logger.info(f\"Loading model from: {model_path}\")\n",
        "        model = load_model(model_path)\n",
        "\n",
        "        # 加载和预处理图像\n",
        "        img = load_img(image_path, target_size=target_size)\n",
        "        img_array = img_to_array(img)\n",
        "        img_array = np.expand_dims(img_array, axis=0) / 255.0\n",
        "\n",
        "        # 预测图像\n",
        "        predictions = model.predict(img_array)\n",
        "        predicted_class = np.argmax(predictions, axis=1)\n",
        "        prob_class = {f\"class_{i}\": prob for i, prob in enumerate(predictions[0])}\n",
        "\n",
        "        # 将结果写入文件\n",
        "        with open(output_file, \"w\") as file:\n",
        "            file.write(f\"Predicted Class: {predicted_class[0]}\\n\")\n",
        "            file.write(\"Probabilities:\\n\")\n",
        "            file.write(json.dumps(prob_class, indent=4))\n",
        "\n",
        "        return predicted_class[0], prob_class\n",
        "\n",
        "    except Exception as e:\n",
        "        # 记录错误\n",
        "        logger.error(f\"Error during model prediction: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# 使用示例\n",
        "predict_classname, prob_class = predict_image_class(\n",
        "    model_path=\"path/to/your/model.h5\",\n",
        "    image_path=\"path/to/your/image.jpg\",\n",
        "    output_file=\"prediction_results.txt\"\n",
        ")"
      ],
      "metadata": {
        "id": "hV3QRk3MYJw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 然後在這個function中我還要再創一個log檔(使用daily_logger)來記錄，(1)輸入的model_path 還有預測的時間, (2)model如果有出錯的話的error  "
      ],
      "metadata": {
        "id": "ab61-14zeTcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import logging\n",
        "import datetime\n",
        "\n",
        "def predict_image_class(model_path, image_path, output_txt_path):\n",
        "    logger = daily_logger()\n",
        "\n",
        "    try:\n",
        "        # 载入模型\n",
        "        model = tf.keras.models.load_model(model_path)\n",
        "        logger.info(f\"Model loaded from {model_path}\")\n",
        "\n",
        "        # 处理图像\n",
        "        img = tf.keras.preprocessing.image.load_img(image_path, target_size=(224, 224))\n",
        "        img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "        img_array = np.expand_dims(img_array, axis=0) / 255.0\n",
        "\n",
        "        # 预测\n",
        "        predictions = model.predict(img_array)\n",
        "        predicted_class_index = np.argmax(predictions, axis=1)[0]\n",
        "        class_names = list(model.class_indices.keys())  # 需要模型的类别索引\n",
        "        predicted_classname = class_names[predicted_class_index]\n",
        "\n",
        "        # 构建预测概率的字典\n",
        "        prob_class = {class_names[i]: float(predictions[0, i]) for i in range(len(class_names))}\n",
        "\n",
        "        # 记录预测时间\n",
        "        current_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "        logger.info(f\"Prediction made at {current_time}\")\n",
        "\n",
        "        # 将预测结果保存到文本文件\n",
        "        with open(output_txt_path, 'w') as file:\n",
        "            file.write(f\"Predicted class: {predicted_classname}\\n\")\n",
        "            file.write(\"Probabilities:\\n\")\n",
        "            for classname, prob in prob_class.items():\n",
        "                file.write(f\"{classname}: {prob}\\n\")\n",
        "\n",
        "        return predicted_classname, prob_class\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during model prediction: {e}\")\n",
        "        raise\n",
        "\n",
        "# 使用示例\n",
        "predicted_classname, prob_class = predict_image_class(\n",
        "    model_path=\"path/to/your/model.h5\",\n",
        "    image_path=\"path/to/your/image.jpg\",\n",
        "    output_txt_path=\"path/to/your/output.txt\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "4L6vaJWmeWAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 創一個log檔(使用daily_logger)來記錄，輸入的model_path 還有預測的時間 (class_name有data_frame的csv檔)"
      ],
      "metadata": {
        "id": "HdL7sZGpenYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def predict_image_class(model_path, image_path, class_names_csv, output_txt_path):\n",
        "    logger = daily_logger()\n",
        "\n",
        "    # 记录模型路径和预测时间\n",
        "    logger.info(f\"Predicting image class. Model: {model_path}\")\n",
        "    prediction_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "    logger.info(f\"Prediction time: {prediction_time}\")\n",
        "\n",
        "    # 从 CSV 文件读取类别名称\n",
        "    class_names_df = pd.read_csv(class_names_csv)\n",
        "    class_names = class_names_df.iloc[:, 0].tolist()  # 假设类别名称在第一列\n",
        "\n",
        "    # 加载模型\n",
        "    model = load_model(model_path)\n",
        "\n",
        "    # 加载并预处理图像\n",
        "    img = load_img(image_path, target_size=(224, 224))\n",
        "    img_array = img_to_array(img) / 255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "    # 预测\n",
        "    predictions = model.predict(img_array)\n",
        "    predicted_class_index = np.argmax(predictions[0])\n",
        "    prob_class = {class_names[i]: float(prob) for i, prob in enumerate(predictions[0])}\n",
        "\n",
        "    # 获取预测类别名称\n",
        "    predicted_classname = class_names[predicted_class_index]\n",
        "\n",
        "    # 记录到文本文件\n",
        "    with open(output_txt_path, 'w') as file:\n",
        "        file.write(f\"Predicted Class: {predicted_classname}\\n\")\n",
        "        file.write(f\"Probability of Each Class: {prob_class}\\n\")\n",
        "\n",
        "    return predicted_classname, prob_class\n",
        "\n",
        "# 使用示例\n",
        "model_path = 'path/to/your/model.h5'\n",
        "image_path = 'path/to/your/image.jpg'\n",
        "class_names_csv = 'path/to/your/class_names.csv'\n",
        "output_txt_path = 'path/to/your/output.txt'\n",
        "predict_classname, prob_class = predict_image_class(model_path, image_path, class_names_csv, output_txt_path)\n"
      ],
      "metadata": {
        "id": "DUjuurcNf1y0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}