{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "14_pKA-8d7nAXYEsETJert0XUjzdeJNJh",
      "authorship_tag": "ABX9TyOe4BSFc1l6DSrPSjiYYQiw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Annie00000/Project/blob/main/1_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import time\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.utils import Sequence\n",
        "\n"
      ],
      "metadata": {
        "id": "lXdD8qIqh70O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cpu\n",
        "import os\n",
        "\n",
        "cpu_cores = os.cpu_count()\n",
        "print(\"CPU cores:\", cpu_cores)\n",
        "\n",
        "\n",
        "# GPU\n",
        "import tensorflow as tf\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    for gpu in gpus:\n",
        "        print(gpu)\n",
        "else:\n",
        "    print(\"No GPU found\")"
      ],
      "metadata": {
        "id": "g61-szcU-Y-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data augmentation (自定義數據增強)"
      ],
      "metadata": {
        "id": "Umq46RPF3qFF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 不使用augmentation_dict,固定做隨機旋轉10度"
      ],
      "metadata": {
        "id": "Q0A2f89W6gX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.ndimage import rotate\n",
        "\n",
        "class CustomDataGenerator(Sequence):\n",
        "    def __init__(self, image_paths, labels, batch_size, target_size, label_to_index, num_classes, shuffle=True):\n",
        "        self.image_paths = np.array(image_paths)\n",
        "        self.labels = np.array(labels)\n",
        "        self.batch_size = batch_size\n",
        "        self.target_size = target_size\n",
        "        #self.augmentation_dict = augmentation_dict\n",
        "        self.label_to_index = label_to_index\n",
        "        self.num_classes = num_classes\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.image_paths) / float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_images = []\n",
        "        batch_labels = []\n",
        "\n",
        "        for i in batch_indices:\n",
        "            img_path = self.image_paths[i]\n",
        "            label = self.labels[i]\n",
        "\n",
        "            # 加载和预处理图像 (直接在 TensorFlow 中載入和解碼映像。這意味著從一開始圖像就是以 Tensor 的形式存在的)\n",
        "            img = tf.io.read_file(img_path)\n",
        "            img = tf.image.decode_image(img, channels=3)\n",
        "            img = tf.image.resize(img, self.target_size)\n",
        "\n",
        "            # 应用数据增强\n",
        "            img = self.apply_augmentation(img, label)/ 255 # 規一化\n",
        "            batch_images.append(img)\n",
        "            batch_labels.append(self.label_to_index[label])\n",
        "\n",
        "        return tf.convert_to_tensor(batch_images), to_categorical(batch_labels, num_classes=self.num_classes)\n",
        "\n",
        "    def apply_augmentation(self, image):\n",
        "        # 随机旋转（正负 10 度）\n",
        "        rotation_degree = random.uniform(-10, 10) #np.random.uniform(-10, 10)\n",
        "        image = rotate(image, rotation_degree, reshape=False, mode='nearest')\n",
        "          # reshape=False 保证旋转后的图像大小不变，但这可能导致图像的一部分被裁剪\n",
        "          # mode 决定了在旋转过程中如何处理图像边界之外的像素\n",
        "\n",
        "        # 对比度增强\n",
        "        contrast_factor = 1.5  # 可以根据需要调整这个值\n",
        "        image = self.adjust_contrast(image, contrast_factor)\n",
        "\n",
        "        return image\n",
        "\n",
        "    def adjust_contrast(self, image, contrast_factor):\n",
        "        \"\"\"调整图像的对比度\"\"\"\n",
        "        mean = np.mean(image, axis=(0, 1), keepdims=True)\n",
        "        adjusted = (image - mean) * contrast_factor + mean\n",
        "        return np.clip(adjusted, 0, 255)\n",
        "\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.indices = np.arange(len(self.image_paths))\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indices)\n",
        "\n",
        "\n",
        "\n",
        "# 创建数据生成器实例\n",
        "train_generator = CustomDataGenerator(\n",
        "    train_paths, train_labels, batch_size=32, target_size=(224, 224),\n",
        "     label_to_index=label_to_index,\n",
        "    num_classes=len(unique_labels), shuffle=True\n",
        ")\n",
        "val_generator = CustomDataGenerator(\n",
        "    val_paths, val_labels, batch_size=32, target_size=(224, 224),\n",
        "    label_to_index=label_to_index,\n",
        "    num_classes=len(unique_labels), shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "LimZfULa6meR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## subprogram"
      ],
      "metadata": {
        "id": "taGrBAB3Qz8h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* class name"
      ],
      "metadata": {
        "id": "azn6U9J5rES2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 从训练数据中提取\n",
        "data_dir = '/path/to/training/data'\n",
        "class_labels = [folder_name for folder_name in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, folder_name))]\n"
      ],
      "metadata": {
        "id": "8dQhe2tUrGDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. train model"
      ],
      "metadata": {
        "id": "FuR8nP4fSnb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrained_model():\n",
        ""
      ],
      "metadata": {
        "id": "caBNPZ3cSty-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import datetime\n",
        "import tensorflow as tf\n",
        "\n",
        "def retrain_model(train_data, test_data, model, model_name, model_save_path, log_file_path):\n",
        "    \"\"\"\n",
        "    重新训练模型并保存结果。\n",
        "\n",
        "    :param train_data: 训练数据。\n",
        "    :param test_data: 测试数据。\n",
        "    :param model: 要训练的模型。\n",
        "    :param model_name: 模型名称。\n",
        "    :param model_save_path: 模型保存路径。\n",
        "    :param log_file_path: 日志文件保存路径。\n",
        "    \"\"\"\n",
        "    # 训练模型\n",
        "    model.fit(train_data, validation_data=val_data, epochs=10, callbacks=callbacks)\n",
        "\n",
        "\n",
        "    # 评估模型\n",
        "    test_loss, test_accuracy = model.evaluate(test_data)\n",
        "\n",
        "    # 保存模型 (不需要的話可以改動)\n",
        "    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    model_filename = f\"{model_name}_{timestamp}.h5\"\n",
        "    model.save(os.path.join(model_save_path, model_filename))\n",
        "\n",
        "    # 记录到日志文件 ('a'為追加，'w'為覆蓋原始寫的)(詢問一下要覆蓋還是追加)\n",
        "    with open(log_file_path, 'a') as log_file:\n",
        "        log_file.write(f\"{model_filename}: Test Accuracy = {test_accuracy}\\n\")\n",
        "\n",
        "    return test_accuracy, model_filename\n"
      ],
      "metadata": {
        "id": "SfTUEHMzX_b8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datetime import datetime\n",
        "\n",
        "def retrain_model(data_dir, model_path, model_name, log_file_path, batch_size, target_size, augmentation_dict, label_to_index, num_classes, test_size=0.2):\n",
        "    # 读取所有影像路径和标签\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "\n",
        "    for class_folder in os.listdir(data_dir):\n",
        "        class_folder_path = os.path.join(data_dir, class_folder)\n",
        "        for file in os.listdir(class_folder_path):\n",
        "            fpath = os.path.join(class_folder_path, file)\n",
        "            image_paths.append(fpath)\n",
        "            labels.append(class_folder)\n",
        "\n",
        "    # 切分成训练集和验证集\n",
        "    train_paths, val_paths, train_labels, val_labels = train_test_split(image_paths, labels, test_size=test_size, stratify=labels, shuffle=True, random_state=42)\n",
        "\n",
        "    # 创建数据生成器实例\n",
        "    train_generator = CustomDataGenerator(\n",
        "        train_paths, train_labels, batch_size=batch_size, target_size=target_size,\n",
        "        augmentation_dict=augmentation_dict, label_to_index=label_to_index,\n",
        "        num_classes=num_classes, shuffle=True, apply_clahe=True\n",
        "    )\n",
        "    val_generator = CustomDataGenerator(\n",
        "        val_paths, val_labels, batch_size=batch_size, target_size=target_size,\n",
        "        augmentation_dict={}, label_to_index=label_to_index,\n",
        "        num_classes=num_classes, shuffle=False, apply_clahe=False\n",
        "    )\n",
        "\n",
        "    # 加载模型\n",
        "    model = load_model(model_path)\n",
        "\n",
        "    # 重新训练模型\n",
        "    model.fit(\n",
        "        train_generator,\n",
        "        validation_data=val_generator,\n",
        "        epochs=10,\n",
        "        callbacks=[early_stopping, model_checkpoint, reduce_lr]\n",
        "    )\n",
        "\n",
        "    # 评估模型\n",
        "    test_loss, test_accuracy = model.evaluate(val_generator)\n",
        "\n",
        "    # 保存模型\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    saved_model_name = f\"{model_name}_{timestamp}.h5\"\n",
        "    model.save(saved_model_name)\n",
        "\n",
        "    # 记录到日志文件\n",
        "    with open(log_file_path, 'a') as log_file:\n",
        "        log_file.write(f\"{timestamp}: Test Accuracy = {test_accuracy}\\n\")\n",
        "\n",
        "    return test_accuracy, saved_model_name"
      ],
      "metadata": {
        "id": "r2xLMam2nA0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. predict"
      ],
      "metadata": {
        "id": "9kzBbhtwSr2p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 判断给定路径的文件夹内是否包含其他子文件夹。"
      ],
      "metadata": {
        "id": "UTaLRVdsrdaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def predict_images_based_on_folder_structure(folder_path, model, target_size, class_labels):\n",
        "    \"\"\"\n",
        "    根据文件夹结构预测图片。\n",
        "\n",
        "    :param folder_path: 文件夹路径。\n",
        "    :param model: 预训练的模型。\n",
        "    :param target_size: 图像目标尺寸。\n",
        "    :param class_labels: 类别标签列表。\n",
        "    :return: 预测结果。\n",
        "    \"\"\"\n",
        "    # 检查是否存在子文件夹\n",
        "    contains_subfolders = any(os.path.isdir(os.path.join(folder_path, item)) for item in os.listdir(folder_path))\n",
        "\n",
        "    if contains_subfolders:\n",
        "        # 如果存在子文件夹，使用多文件夹预测函数\n",
        "        return predict_multiple_folders(folder_path, model, target_size, class_labels)\n",
        "    else:\n",
        "        # 否则，使用单文件夹预测函数\n",
        "        return predict_single_folder(folder_path, model, target_size, class_labels)\n"
      ],
      "metadata": {
        "id": "WMvJQuuerauQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 單個文件夾"
      ],
      "metadata": {
        "id": "duJKzDH1jRUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_single_folder(folder_path, model_path):\n",
        "    model = load_model(model_path)\n",
        "\n",
        "    predictions = {}\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.lower().endswith('.png'):\n",
        "            img_path = os.path.join(folder_path, filename)\n",
        "            img = load_and_preprocess_image(img_path)\n",
        "            pred = model.predict(np.expand_dims(img, axis=0))\n",
        "            predicted_class = class_labels[np.argmax(pred)]\n",
        "            predictions[filename] = {\n",
        "                'class': predicted_class,\n",
        "                'probabilities': pred[0].tolist()\n",
        "            }\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "0LNQpjfDjYZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 一個文件夾下有多個文件夾"
      ],
      "metadata": {
        "id": "yyYZE0YHjVIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_multiple_folders(parent_folder_path, model_path):\n",
        "    model = load_model(model_path)\n",
        "    all_predictions = {}\n",
        "    for folder_name in os.listdir(parent_folder_path):\n",
        "        folder_path = os.path.join(parent_folder_path, folder_name)\n",
        "        if os.path.isdir(folder_path):\n",
        "            predictions = predict_single_folder(folder_path, model_path)\n",
        "            all_predictions[folder_name] = predictions\n",
        "    return all_predictions"
      ],
      "metadata": {
        "id": "oL7AjfXjjYGm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}