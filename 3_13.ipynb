{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBOr46AWI0irq1URMZHkqq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Annie00000/Project/blob/main/3_13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 一"
      ],
      "metadata": {
        "id": "zCMAU_5WZGzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  history = model.fit(train_generator,\n",
        "        steps_per_epoch=len(train_paths) // 16,\n",
        "        validation_data=val_generator,\n",
        "        validation_steps=len(val_paths) // 16,\n",
        "        epochs=50,\n",
        "        callbacks=callbacks)"
      ],
      "metadata": {
        "id": "fpywHx7vCEWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (1)"
      ],
      "metadata": {
        "id": "ZaEgnjtzT0EI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "這段程式碼是使用在深度學習框架（如TensorFlow或Keras）中，用於訓練機器學習模型。下面我會逐一解釋各個參數的功能：\n",
        "\n",
        "model.fit(): 這是一個用於訓練模型的方法。它將模型對訓練數據進行多次迭代學習，以最小化損失函數，並進行參數更新。\n",
        "\n",
        "train_generator: 這是一個數據生成器，用於動態產生訓練數據。這種方式有利於處理記憶體無法一次加載所有數據的大規模數據集。它通常會在每個epoch中隨機生成一批訓練數據。\n",
        "\n",
        "steps_per_epoch: 這個參數定義了每個epoch中有多少批次（steps）的數據會被處理。在這個例子中，它被設定為len(train_paths) // 16，意味著每個epoch將處理的批次數量是訓練數據路徑數量除以16的商。這個設定有助於控制每個epoch的訓練時間和學習進度。\n",
        "\n",
        "validation_data: 這是另一個數據生成器，用於動態產生驗證數據。在訓練過程中，這些數據不會用於模型的參數學習，而是用來評估模型的性能，以防止過擬合。\n",
        "\n",
        "validation_steps: 類似於steps_per_epoch，但這是在驗證階段每個epoch後處理的數據批次數量。它被設定為len(val_paths) // 16，代表使用驗證數據路徑數量除以16的商作為每個epoch的驗證步數。\n",
        "\n",
        "epochs: 這個參數指定了訓練過程將運行的總epoch次數，每個epoch都會遍歷一遍所有的訓練數據。在這個例子中，訓練將進行50個epoch。\n",
        "\n",
        "callbacks: 這是一個列表，其中可以包含多個回調函數，這些函數可以在訓練的不同階段被呼叫，用於實現如早停（early stopping）、模型儲存、學習率調整等高級功能。\n",
        "\n",
        "總的來說，這段程式碼展示了一個典型的深度學習模型訓練過程，包括使用數據生成器來處理大型數據集、設定訓練和驗證步數來控制每個epoch的處理量、指定epoch次數來定義訓練持續時間，以及使用回調函數來監控訓練進度和性能。"
      ],
      "metadata": {
        "id": "_Xe6cXXCTy5Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (2)"
      ],
      "metadata": {
        "id": "j2deWSi3T2_4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "如果在自定義的數據生成器中已經通過__len__方法定義了每個epoch的批次數量，那麼在大多數情況下，指定steps_per_epoch參數就不是必須的了。__len__方法的返回值將被用來自動決定每個epoch處理的步驟數（即批次數）。這種設計允許數據生成器靈活地控制每個epoch的迭代次數，而無需在model.fit調用中硬編碼。\n",
        "\n",
        "然而，steps_per_epoch參數仍然可以有其用處：\n",
        "\n",
        "覆蓋：即使數據生成器已經通過__len__定義了步數，steps_per_epoch也可以用來覆蓋這個值。這可能對於實驗目的或特定情況下調整訓練行為很有幫助。\n",
        "\n",
        "靈活性：在某些情況下，你可能希望在不同的訓練階段使用不同的步數。例如，在某些epoch中快速迭代模型（使用較少的步驟），而在其他epoch中更徹底地訓練（使用更多的步驟）。這時，steps_per_epoch參數可以提供這種靈活性。\n",
        "\n",
        "特定場景下的必要性：某些情況下，數據生成器可能因為實現的方式，無法準確地提前知道每個epoch的確切步驟數，特別是在處理非常大的數據集或進行複雜的數據增強時。在這種情況下，直接指定steps_per_epoch可以確保訓練的一致性和可預測性。\n",
        "\n",
        "總之，當你的數據生成器已經定義了__len__方法時，steps_per_epoch參數通常不是必需的，但它仍然提供了一個有用的工具來調整訓練的具體行為，根據你的特定需求和目標。"
      ],
      "metadata": {
        "id": "b-_-xEssCFNV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 二"
      ],
      "metadata": {
        "id": "PtnWpqPIZJhW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "在建構的時候使用accuracy來做評估準則\n",
        "model.compile(optimizer='adam', loss='crossentropy', metric=['accuracy']\n",
        "\n",
        "而在Callback的時候使用 val_loss來做監控，\n",
        "\n",
        "上述兩者一個用accuracy，一邊用val_loss ，這樣可行ˊ嗎? 會有問題嗎"
      ],
      "metadata": {
        "id": "6F-1R9trZUTl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "是的，這樣做是完全可行的，也是機器學習實踐中的常見做法。在訓練深度學習模型時，使用不同的指標來進行模型編譯和回調監控是很正常的。讓我們分開來看這兩個過程：\n",
        "\n",
        "模型編譯時使用的準確度（Accuracy）\n",
        "在模型編譯階段使用準確度（accuracy）作為評估指標，主要是為了在訓練過程中能夠直觀地看到模型對於訓練和驗證數據的分類準確性。準確度是一個直觀的指標，能夠幫助你了解模型在正確分類數據方面的表現如何。\n",
        "\n",
        "回調（Callbacks）中使用的損失值（val_loss）\n",
        "而在設定回調函數時使用驗證集上的損失值（val_loss）作為監控指標，則是基於另一個考量。回調函數，如早期停止（EarlyStopping）、模型檢查點（ModelCheckpoint）、學習率調整（ReduceLROnPlateau）等，通常用於控制訓練過程、防止過擬合和保證模型在驗證數據上的表現。這些函數使用損失值作為監控指標是因為損失值直接反映了模型對於整體數據的預測誤差，並且是訓練過程中優化的直接目標。\n",
        "\n",
        "為什麼這種做法是合理的？\n",
        "補充性：準確度和損失值各自提供了不同的視角來評估模型的性能。準確度更側重於模型預測的正確性，而損失值則反映了模型預測與真實值之間的誤差。這兩種指標一起使用可以給出更全面的性能評估。\n",
        "\n",
        "適用性：在訓練階段關注準確度，有助於直觀了解模型的學習進度。而在訓練控制（如回調函數）中使用損失值，則是因為損失值是優化過程的直接目標，適合用於早停、調整學習率等操作。\n",
        "\n",
        "靈活性：這種做法提供了操作的靈活性，允許開發者根據具體情況選擇最合適的監控指標。\n",
        "\n",
        "總之，根據你的描述，這樣配置模型和回調是沒有問題的，這是一個很標準也很有效的做法，能夠幫助你在保證模型性能的同時，有效控制訓練過程。"
      ],
      "metadata": {
        "id": "pkTVTj5uZtiE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 三."
      ],
      "metadata": {
        "id": "BJCae6wRZ_Ot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Xception(include_top=False, input_shape=(224, 224, 3), weights='imagenet'),\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(128, activation='relu'),\n",
        "    # 随机丢弃40%的神经元输出，以防止模型过拟合。这是通过在训练过程中随机将部分网络节点的输出设置为0来实现的。\n",
        "    Dropout(0.4),\n",
        "    Dense(len(unique_labels), activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "FBDTCLd0ZKm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (1)"
      ],
      "metadata": {
        "id": "gCHOiJQ8bBg-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* GlobalAveragePooling2D()"
      ],
      "metadata": {
        "id": "07ZN6qKWa_uZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "意義:\n",
        "\n",
        "\n",
        "1. 特徵壓縮：GlobalAveragePooling2D()對每個特徵圖（feature map）進行全局平均池化，即計算每個特徵圖上所有元素的平均值。這樣，對於每一個特徵圖，不論其尺寸如何，都會產生一個單一的數值。假如有N個特徵圖，則會得到N個數值。這有效地將高維數據壓縮為低維，同時保留了特徵圖的關鍵信息。\n",
        "\n",
        "2. 減少過擬合：由於這個層大大減少了模型的參數數量（相比於全連接層），它有助於減輕過擬合的問題。在傳統的卷積神經網絡中，全連接層往往包含大量參數，容易引起過擬合。\n",
        "\n",
        "3. 促進特徵的空間關係學習：通過對整個特徵圖的平均，強迫網絡捕捉到全局特徵，而不是只關注特定區域的細節，這在處理圖像分類任務時特別有用，因為類別的判定往往依賴於圖像的整體特徵而非局部細節。\n",
        "\n",
        "------------------------------------------------\n",
        "為什麼放在這裡比較好 ?\n",
        "\n",
        "在此模型結構中，GlobalAveragePooling2D()層被放置在頂層卷積層（Xception模型）之後，這是基於以下幾個考量：\n",
        "\n",
        "1. 簡化模型結構：在頂層卷積層之後直接使用GlobalAveragePooling2D()，可以有效地減少模型參數，相比於使用大型的全連接層，這樣的設計使模型更簡潔，計算成本更低。\n",
        "\n",
        "2. 提高泛化能力：由於參數數量的減少有助於防止過擬合，因此模型的泛化能力會得到提高。這對於在多變的實際應用場景中維持高性能是很重要的。\n",
        "\n",
        "3. 適應不同尺寸的輸入：GlobalAveragePooling2D()使得模型能夠更加靈活地處理不同尺寸的輸入圖像。因為它對特徵圖進行全局池化，不依賴於輸入圖像的具體尺寸，從而允許模型接受任意大小的輸入。\n",
        "\n",
        "總的來說，GlobalAveragePooling2D()層的加入不僅減少了模型的參數數量，降低了過擬合風險，同時也使模型更加適應於不同的應用場景，是進行深度學習圖像分類任務時的一種有效且流行的設計選擇。"
      ],
      "metadata": {
        "id": "OKfN4e6XbjQS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (2)"
      ],
      "metadata": {
        "id": "vC5TYT6XbC83"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "激活函數 先用relu再用softmax，有什麼講究嗎"
      ],
      "metadata": {
        "id": "PlajtR58a3E_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ReLU激活函数:\n",
        "\n",
        "1. f(x)=max(0,x)，它是一种非线性函数，常用于隐藏层中。ReLU的主要优点是计算简单且效率高，同时帮助缓解了梯度消失问题（在训练深度网络时，较深层的梯度可能会变得非常小，难以有效更新权重），使得深度神经网络的训练变得更加可行。\n",
        "\n",
        "2. ReLU函数通过将所有负值置为0，保持正值不变，这样的非线性特性允许模型学习复杂的数据表示。在实践中，ReLU激活在多层神经网络中极为常见，因为它能够有效地促进梯度的反向传播，加速网络的收敛，同时减少计算资源的需求。\n",
        "\n",
        "3. 非线性：虽然ReLU看起来非常简单，但它是非线性的。这使得ReLU可以被用来建构复杂的非线性模型。\n",
        "\n",
        "4. 计算简单：相比于其他激活函数，如Sigmoid或Tanh，ReLU在计算上更为高效。\n",
        "\n",
        "5. 缓解梯度消失问题：在训练深层神经网络时，Sigmoid或Tanh函数容易导致梯度消失问题，而ReLU能在一定程度上缓解这个问题，因为对于正输入，其导数为1。\n",
        "\n",
        "* Softmax激活函数:\n",
        "1. 将一个向量或一组数值转换成概率分布的函数。在分类任务中，Softmax常用于模型的输出层，尤其是在多类分类问题中。Softmax能够确保模型输出的数值被压缩到(0, 1)区间内，并且所有输出值的和为1，这样每个数值就可以被解释为属于对应类别的概率。\n",
        "\n",
        "2. 概率输出：Softmax确保所有输出值都在0到1之间，并且所有输出值的和为1，这使得输出可以被解释为概率分布。\n",
        "\n",
        "3. 适用于多类分类：在多类分类问题中，每个类别都会有一个对应的概率，而Softmax能够有效地为每个类别分配一个概率。\n",
        "\n",
        "\n",
        "\n",
        "* 结合使用ReLU和Softmax\n",
        "1. 在隐藏层使用ReLU：使用ReLU作为隐藏层的激活函数，可以帮助模型学习复杂的特征表示，并保持计算的高效性。ReLU激活函数有助于处理非线性问题，同时避免梯度消失问题，使得网络可以更深、更有效地学习。\n",
        "\n",
        "2. 在输出层使用Softmax：对于多类分类问题，使用Softmax作为输出层的激活函数可以将模型的输出解释为概率分布。这有助于在分类任务中评估每个类别的相对可能性，使得模型的输出更加直观易懂。\n",
        "\n",
        "综上所述，先使用ReLU再使用Softmax的做法是为了在模型内部利用ReLU的非线性特性学习复杂的数据表示，而在模型输出时通过Softmax将这些复杂的表示转换成概率分布，从而进行有效的多类别分类。这种组合是解决多类别分类问题的一种常见和有效的策略。\n",
        "\n",
        "\n",
        "ReLU + Softmax的组合是因为它们在神经网络中扮演的角色互补。ReLU用于增加网络的非线性并帮助网络学习复杂的特征表示，而Softmax用于将这些特征表示转换为一个概率分布，以便于进行多类别的分类判断。\n",
        "这种组合允许神经网络能够处理复杂的特征学习任务，同时在最终输出时给出清晰的概率判断，这对于分类问题特别有用。因此，先用ReLU（或其他非线性激活函数）增加模型的非线性能力和表达能力，再用Softmax进行概率输出，是一种非常典型且有效的设计策略。"
      ],
      "metadata": {
        "id": "fpF0DIAId_K1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 四."
      ],
      "metadata": {
        "id": "IKLkfsNCaCaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "rEzbglWzaEVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Loss（损失函数）：'categorical_crossentropy'\n",
        "损失函数衡量模型预测值与真实值之间的差异，训练过程中的目标就是最小化这个值。\n",
        "Categorical Crossentropy是多分类问题中常用的损失函数，特别是当目标变量表示为独热编码（one-hot encoding）时。例如，如果你的任务是识别图像属于3个类别中的哪一个，每个类别的目标标签会是一个长度为3的向量，其中一个元素为1，其余为0。\n",
        "如果预测值与真实值越接近，交叉熵损失越小。\n",
        "它计算的是实际输出分布和预测输出分布之间的距离，公式涉及到对真实标签的预测概率的对数取反和求和。"
      ],
      "metadata": {
        "id": "vdZESh8iiyDS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 优化器\n",
        "\n",
        "用来更新网络中权重以减少损失函数值的算法。优化器决定了学习速率（learning rate）以及如何移动权重以达到最低损失值。\n",
        "\n",
        "Adam是一种自适应学习率优化算法，它被广泛用于训练深度学习模型。Adam结合了Momentum和RMSprop两种优化算法的优点：\n",
        "\n",
        "1. Momentum考虑了过去梯度的指数衰减平均，帮助加速学习过程，并且能够在相关方向上更快地达到最优。\n",
        "2. RMSprop则调整学习率以便在不同方向上以不同速度进行学习，这有助于解决Adagrad学习率递减得过快的问题。\n",
        "\n",
        "Adam自动调整学习率，减少了学习率选择的复杂性，是一个效率较高的训练算法。"
      ],
      "metadata": {
        "id": "Lc2CvPtOjk9n"
      }
    }
  ]
}