{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "14_pKA-8d7nAXYEsETJert0XUjzdeJNJh",
      "authorship_tag": "ABX9TyM1Oe6Z+aJRxAu40JVhWXEV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Annie00000/Project/blob/main/1_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import time\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.utils import Sequence\n",
        "\n"
      ],
      "metadata": {
        "id": "lXdD8qIqh70O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cpu\n",
        "import os\n",
        "\n",
        "cpu_cores = os.cpu_count()\n",
        "print(\"CPU cores:\", cpu_cores)\n",
        "\n",
        "\n",
        "# GPU\n",
        "import tensorflow as tf\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    for gpu in gpus:\n",
        "        print(gpu)\n",
        "else:\n",
        "    print(\"No GPU found\")"
      ],
      "metadata": {
        "id": "g61-szcU-Y-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data augmentation (自定義數據增強)"
      ],
      "metadata": {
        "id": "Umq46RPF3qFF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 不使用augmentation_dict,固定做隨機旋轉10度"
      ],
      "metadata": {
        "id": "Q0A2f89W6gX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.ndimage import rotate\n",
        "\n",
        "class CustomDataGenerator(Sequence):\n",
        "    def __init__(self, image_paths, labels, batch_size, target_size, label_to_index, num_classes, shuffle=True):\n",
        "        self.image_paths = np.array(image_paths)\n",
        "        self.labels = np.array(labels)\n",
        "        self.batch_size = batch_size\n",
        "        self.target_size = target_size\n",
        "        #self.augmentation_dict = augmentation_dict\n",
        "        self.label_to_index = label_to_index\n",
        "        self.num_classes = num_classes\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.image_paths) / float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_images = []\n",
        "        batch_labels = []\n",
        "\n",
        "        for i in batch_indices:\n",
        "            img_path = self.image_paths[i]\n",
        "            label = self.labels[i]\n",
        "\n",
        "            # 加载和预处理图像 (直接在 TensorFlow 中載入和解碼映像。這意味著從一開始圖像就是以 Tensor 的形式存在的)\n",
        "            img = tf.io.read_file(img_path)\n",
        "            img = tf.image.decode_image(img, channels=3)\n",
        "            img = tf.image.resize(img, self.target_size)\n",
        "\n",
        "            # 应用数据增强\n",
        "            img = self.apply_augmentation(img, label)/ 255 # 規一化\n",
        "            batch_images.append(img)\n",
        "            batch_labels.append(self.label_to_index[label])\n",
        "\n",
        "        return tf.convert_to_tensor(batch_images), to_categorical(batch_labels, num_classes=self.num_classes)\n",
        "\n",
        "    def apply_augmentation(self, image):\n",
        "        # 随机旋转（正负 10 度）\n",
        "        rotation_degree = random.uniform(-10, 10) #np.random.uniform(-10, 10)\n",
        "        image = rotate(image, rotation_degree, reshape=False, mode='nearest')\n",
        "          # reshape=False 保证旋转后的图像大小不变，但这可能导致图像的一部分被裁剪\n",
        "          # mode 决定了在旋转过程中如何处理图像边界之外的像素\n",
        "\n",
        "        # 对比度增强\n",
        "        contrast_factor = 1.5  # 可以根据需要调整这个值\n",
        "        image = self.adjust_contrast(image, contrast_factor)\n",
        "\n",
        "        return image\n",
        "\n",
        "    def adjust_contrast(self, image, contrast_factor):\n",
        "        \"\"\"调整图像的对比度\"\"\"\n",
        "        mean = np.mean(image, axis=(0, 1), keepdims=True)\n",
        "        adjusted = (image - mean) * contrast_factor + mean\n",
        "        return np.clip(adjusted, 0, 255)\n",
        "\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.indices = np.arange(len(self.image_paths))\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indices)\n",
        "\n",
        "\n",
        "\n",
        "# 创建数据生成器实例\n",
        "train_generator = CustomDataGenerator(\n",
        "    train_paths, train_labels, batch_size=32, target_size=(224, 224),\n",
        "     label_to_index=label_to_index,\n",
        "    num_classes=len(unique_labels), shuffle=True\n",
        ")\n",
        "val_generator = CustomDataGenerator(\n",
        "    val_paths, val_labels, batch_size=32, target_size=(224, 224),\n",
        "    label_to_index=label_to_index,\n",
        "    num_classes=len(unique_labels), shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "LimZfULa6meR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logger"
      ],
      "metadata": {
        "id": "2b4225PAkjzL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 補充: sys.path.insert(1, '../../path') 是Python中一种修改模块搜索路径的用法"
      ],
      "metadata": {
        "id": "I2Ipd9Psp3W_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def daily_logger():\n",
        "  local_time = datetime.datetime.now()\n",
        "  pid = os.getpid()\n",
        "  with open('../../../../../sever_ip','r') as file:\n",
        "    server_IP = file.readline().strip()\n",
        "  #server_IP = socket.gethostbyname(socket.gethostname()) # 獲取當前主機的IP地址\n",
        "  date = local_time.strftime('%Y-%m-%d')\n",
        "  if 'log' not in os.listdir('./'):\n",
        "    os.mkdir('./log')\n",
        "  logger = logging.getLogger(f'{date}__log') # 创建或获取一个名为 '{date}__log' 的日志记录器（logger）对象，并将其赋值给变量 logger\n",
        "  if not logger.handlers:\n",
        "    handler = logging.FileHandler(f'./log/{date}.txt')\n",
        "    handler.setLevel(logging.INFO)\n",
        "    formatter = logging.Formater(f'[%(asctime)s__{server_IP}__{pid}]:%(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
        "    handler.setFormatter(formatter) # 将上面创建的格式化器应用到文件处理器(handler)上。 !!!!! 增加這行!!!!!\n",
        "    logger.addHandler(handler)\n",
        "  return logger"
      ],
      "metadata": {
        "id": "WRZ6qw-tkp9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def daily_logger():\n",
        "  local_time = datetime.datetime.now()\n",
        "  pid = os.getpid() #取得目前進程的進程ID（PID） (Process Identifier)\n",
        "  with open('../../../../../sever_ip','r') as file:\n",
        "    server_IP = file.readline().strip() # strip:去除行尾的空白字符\n",
        "  #server_IP = socket.gethostbyname(socket.gethostname()) # 獲取當前主機的IP地址\n",
        "  user = 'system'\n",
        "  date = local_time.strftime('%Y-%m-%d') # 將變數 local_time 格式化為 'YYYY-MM-DD' 格式的字串\n",
        "  if 'log' not in os.listdir('./'):  # 检查当前目录（.）下是否有名为 'log' 的目录。如果没有，创建这个目录。\n",
        "    os.mkdir('./log')\n",
        "  logger = logging.getLogger(f'{date}__log') # 创建或获取一个名为 '{date}__log' 的日志记录器（logger）对象，并将其赋值给变量 logger\n",
        "\n",
        "  if not logger.handlers: # 检查 logger 是否没有任何处理器（handlers）\n",
        "    handler = logging.FileHandler(f'./log/{date}.txt')  # 建立一個新的檔案處理器（FileHandler），用於將日誌寫入到路徑為 './log/{date}.txt' 的文件\n",
        "    handler.setLevel(logging.INFO)\n",
        "    formatter = logging.Formatter(f'[%(asctime)s__{server_IP}__{pid}]:{server_IP}-{user}, %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
        "    # 创建一个格式化器（Formatter），用于定义日志消息的格式。这里的格式包括时间戳、服务器IP、进程ID、用户和日志消息。\n",
        "    handler.setFormatter(formatter) # 将上面创建的格式化器应用到文件处理器(handler)上。\n",
        "    logger.addHandler(handler) # 将文件处理器(handler) 添加到日志记录器 logger 上。\n",
        "  return logger"
      ],
      "metadata": {
        "id": "2SXHNAwOklVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## sys.path.insert(1, '...')"
      ],
      "metadata": {
        "id": "7Utq_cKiqDfP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. sys.path 列表：\n",
        "\n",
        "  sys.path 是一个字符串列表，用于指定解释器搜索模块的目录。默认情况下，它包含当前脚本的目录和Python的安装目录。\n",
        "2. sys.path.insert(index, path) 方法：\n",
        "\n",
        "  * 此方法用于在 sys.path 列表的指定索引位置插入一个新的路径。在这种情况下，'../../path' 是您希望添加的新路径。\n",
        "  * index 参数 1 意味着新路径被插入在列表的开始位置，紧随原始的第一个条目之后。这确保了在默认目录之前搜索您指定的目录。\n",
        "\n",
        "3. 路径 '../../path'：\n",
        "\n",
        "  * '../../path' 是一个相对路径。这个路径是相对于当前脚本运行目录的上上级目录中的 path 目录。\n",
        "  * 例如，如果您的脚本位于 /home/user/projects/myproject/scripts 目录下，../../path 将解析为 /home/user/path。\n",
        "4. 用途：\n",
        "\n",
        "  * 这种做法通常用于临时添加项目的特定目录到模块搜索路径中，特别是当您希望导入不在标准模块搜索路径中的模块时。\n",
        "  * 它允许您在不修改环境变量的情况下，临时扩展解释器的模块搜索范围。\n",
        "5. 注意事项：\n",
        "\n",
        "  * 修改 sys.path 可能会对模块的导入顺序产生影响，有时可能导致意外的行为，特别是如果存在同名模块的情况。\n",
        "  * 在更大的项目中，更稳妥的方法是使用虚拟环境和适当的包结构来管理模块。"
      ],
      "metadata": {
        "id": "Tdd9xC1-qJGn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 紀錄log以及分段執行时间"
      ],
      "metadata": {
        "id": "n5nDq8Yxr1No"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 情況一"
      ],
      "metadata": {
        "id": "60zLklAyuY5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import time\n",
        "\n",
        "# 设置日志记录器\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')\n",
        "logger = logging.getLogger(__name__) # __name__ 变量包含了模块的名字，正在创建一个与当前模块名相关联的日志记录器。使得日志消息更容易跟踪到特定的模块。\n",
        "\n",
        "def your_function():\n",
        "    # 记录函数开始时间\n",
        "    start_time = time.time()\n",
        "\n",
        "    # 这里是您的代码逻辑\n",
        "    time.sleep(2)  # 举例，模拟执行时间\n",
        "\n",
        "    # 记录函数结束时间\n",
        "    end_time = time.time()\n",
        "\n",
        "    # 计算并记录执行时间\n",
        "    elapsed_time = end_time - start_time\n",
        "    logger.info(f'your_function 执行耗时: {elapsed_time} 秒')\n",
        "\n",
        "def another_function():\n",
        "    # 类似地，为其他函数执行计时\n",
        "    start_time = time.time()\n",
        "\n",
        "    # 模拟一些操作\n",
        "    time.sleep(1)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    logger.info(f'another_function 执行耗时: {elapsed_time} 秒')\n",
        "\n",
        "# 调用函数\n",
        "your_function()\n",
        "another_function()"
      ],
      "metadata": {
        "id": "u55dxNFwr52_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 情況二"
      ],
      "metadata": {
        "id": "fXiUvXb-ua0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### 設置log ###\n",
        "import logging\n",
        "import os\n",
        "\n",
        "# 创建一个logger\n",
        "logger = logging.getLogger('my_automation_system')\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# 创建一个handler，用于写入日志文件\n",
        "log_file = './my_automation_system.log'\n",
        "if not os.path.exists(os.path.dirname(log_file)):\n",
        "    os.makedirs(os.path.dirname(log_file))\n",
        "file_handler = logging.FileHandler(log_file)\n",
        "\n",
        "# 设置日志记录格式\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "file_handler.setFormatter(formatter)\n",
        "\n",
        "# 将handler添加到logger\n",
        "logger.addHandler(file_handler)"
      ],
      "metadata": {
        "id": "22IpWbQbucuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 紀錄時間  ###\n",
        "import time\n",
        "\n",
        "def log_execution_time(func):\n",
        "    \"\"\"一个装饰器，用于测量函数执行时间并记录到日志\"\"\"\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start_time = time.time()\n",
        "        result = func(*args, **kwargs)\n",
        "        end_time = time.time()\n",
        "        logger.info(f\"执行 {func.__name__} 耗时 {end_time - start_time} 秒\")\n",
        "        return result\n",
        "    return wrapper\n",
        "\n",
        "# 示例：使用装饰器来测量函数的执行时间\n",
        "@log_execution_time\n",
        "def my_task():\n",
        "    # 模拟一些工作\n",
        "    time.sleep(2)\n",
        "    print(\"任务完成\")\n",
        "\n",
        "my_task()"
      ],
      "metadata": {
        "id": "cErFduK8uhmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们定义了一个装饰器 log_execution_time，它可以被应用于任何函数上。当被装饰的函数执行时，该装饰器会计算并记录该函数的执行时间。\n",
        "\n",
        "这种方法非常适合于自动化系统，其中您可能需要跟踪多个任务的执行时间，同时保持日志记录的一致性和准确性。\n",
        "\n",
        "请根据您的实际需求调整日志记录的细节，比如日志级别、日志格式或日志文件的位置。"
      ],
      "metadata": {
        "id": "drFgUII-uq8I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## subprogram"
      ],
      "metadata": {
        "id": "taGrBAB3Qz8h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* class name"
      ],
      "metadata": {
        "id": "azn6U9J5rES2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 从训练数据中提取\n",
        "data_dir = '/path/to/training/data'\n",
        "class_labels = [folder_name for folder_name in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, folder_name))]\n"
      ],
      "metadata": {
        "id": "8dQhe2tUrGDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. train model"
      ],
      "metadata": {
        "id": "FuR8nP4fSnb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrained_model():\n"
      ],
      "metadata": {
        "id": "caBNPZ3cSty-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import datetime\n",
        "import tensorflow as tf\n",
        "\n",
        "def retrain_model(train_data, test_data, model, model_name, model_save_path, log_file_path):\n",
        "    \"\"\"\n",
        "    重新训练模型并保存结果。\n",
        "\n",
        "    :param train_data: 训练数据。\n",
        "    :param test_data: 测试数据。\n",
        "    :param model: 要训练的模型。\n",
        "    :param model_name: 模型名称。\n",
        "    :param model_save_path: 模型保存路径。\n",
        "    :param log_file_path: 日志文件保存路径。\n",
        "    \"\"\"\n",
        "    # 训练模型\n",
        "    model.fit(train_data, validation_data=val_data, epochs=10, callbacks=callbacks)\n",
        "\n",
        "\n",
        "    # 评估模型\n",
        "    test_loss, test_accuracy = model.evaluate(test_data)\n",
        "\n",
        "    # 保存模型 (不需要的話可以改動)\n",
        "    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    model_filename = f\"{model_name}_{timestamp}.h5\"\n",
        "    model.save(os.path.join(model_save_path, model_filename))\n",
        "\n",
        "    # 记录到日志文件 ('a'為追加，'w'為覆蓋原始寫的)(詢問一下要覆蓋還是追加)\n",
        "    with open(log_file_path, 'a') as log_file:\n",
        "        log_file.write(f\"{model_filename}: Test Accuracy = {test_accuracy}\\n\")\n",
        "\n",
        "    return test_accuracy, model_filename\n"
      ],
      "metadata": {
        "id": "SfTUEHMzX_b8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datetime import datetime\n",
        "\n",
        "def retrain_model(data_dir, model_path, model_name, log_file_path, batch_size, target_size, augmentation_dict, label_to_index, num_classes, test_size=0.2):\n",
        "    # 读取所有影像路径和标签\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "\n",
        "    for class_folder in os.listdir(data_dir):\n",
        "        class_folder_path = os.path.join(data_dir, class_folder)\n",
        "        for file in os.listdir(class_folder_path):\n",
        "            fpath = os.path.join(class_folder_path, file)\n",
        "            image_paths.append(fpath)\n",
        "            labels.append(class_folder)\n",
        "\n",
        "    # 切分成训练集和验证集\n",
        "    train_paths, val_paths, train_labels, val_labels = train_test_split(image_paths, labels, test_size=test_size, stratify=labels, shuffle=True, random_state=42)\n",
        "\n",
        "    # 创建数据生成器实例\n",
        "    train_generator = CustomDataGenerator(\n",
        "        train_paths, train_labels, batch_size=batch_size, target_size=target_size,\n",
        "        augmentation_dict=augmentation_dict, label_to_index=label_to_index,\n",
        "        num_classes=num_classes, shuffle=True, apply_clahe=True\n",
        "    )\n",
        "    val_generator = CustomDataGenerator(\n",
        "        val_paths, val_labels, batch_size=batch_size, target_size=target_size,\n",
        "        augmentation_dict={}, label_to_index=label_to_index,\n",
        "        num_classes=num_classes, shuffle=False, apply_clahe=False\n",
        "    )\n",
        "\n",
        "    # 加载模型\n",
        "    model = load_model(model_path)\n",
        "\n",
        "    # 重新训练模型\n",
        "    model.fit(\n",
        "        train_generator,\n",
        "        validation_data=val_generator,\n",
        "        epochs=10,\n",
        "        callbacks=[early_stopping, model_checkpoint, reduce_lr]\n",
        "    )\n",
        "\n",
        "    # 评估模型\n",
        "    test_loss, test_accuracy = model.evaluate(val_generator)\n",
        "\n",
        "    # 保存模型\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    saved_model_name = f\"{model_name}_{timestamp}.h5\"\n",
        "    model.save(saved_model_name)\n",
        "\n",
        "    # 记录到日志文件\n",
        "    with open(log_file_path, 'a') as log_file:\n",
        "        log_file.write(f\"{timestamp}: Test Accuracy = {test_accuracy}\\n\")\n",
        "\n",
        "    return test_accuracy, saved_model_name"
      ],
      "metadata": {
        "id": "r2xLMam2nA0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_data(data_dir):\n",
        "    \"\"\"\n",
        "    从指定的文件夹加载图像路径和标签。\n",
        "\n",
        "    :param data_dir: 包含图像的文件夹的路径。\n",
        "    :return: 图像路径列表和相应的标签列表。\n",
        "    \"\"\"\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "\n",
        "    # 遍历文件夹中的所有子文件夹\n",
        "    for class_folder_name in os.listdir(data_dir):\n",
        "        class_folder_path = os.path.join(data_dir, class_folder_name)\n",
        "\n",
        "        # 确保它是一个文件夹\n",
        "        if os.path.isdir(class_folder_path):\n",
        "            # 遍历文件夹中的所有图像文件\n",
        "            for image_file in os.listdir(class_folder_path):\n",
        "                image_path = os.path.join(class_folder_path, image_file)\n",
        "\n",
        "                # 只处理图像文件\n",
        "                if image_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    image_paths.append(image_path)\n",
        "                    labels.append(class_folder_name)\n",
        "\n",
        "    return image_paths, labels"
      ],
      "metadata": {
        "id": "kOm5eNTU-jG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 步骤 1：创建模块文件"
      ],
      "metadata": {
        "id": "obV0CZ13dNY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# image_classification_trainer.py\n",
        "\n",
        "import os\n",
        "import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "\n",
        "class ImageClassificationTrainer:\n",
        "    def __init__(self, data_dir, model, model_save_path, log_file_path):\n",
        "        self.data_dir = data_dir\n",
        "        self.model = model\n",
        "        self.model_save_path = model_save_path\n",
        "        self.log_file_path = log_file_path\n",
        "\n",
        "    def load_data(self):\n",
        "\n",
        "        pass\n",
        "\n",
        "    def retrained_model(self, test_data_generator):\n",
        "        # 读取数据并拆分训练/验证集\n",
        "        image_paths, labels = self.load_data()\n",
        "        train_paths, val_paths, train_labels, val_labels = train_test_split(image_paths, labels, test_size=0.2, stratify=labels, shuffle=True, random_state=42)\n",
        "\n",
        "        # 创建训练和验证数据生成器\n",
        "        train_generator = CustomDataGenerator(train_paths, train_labels, ...)\n",
        "        val_generator = CustomDataGenerator(val_paths, val_labels, ...)\n",
        "\n",
        "        # 训练模型\n",
        "        self.model.fit(train_generator, validation_data=val_generator, epochs=10, ...)\n",
        "\n",
        "        # 评估模型\n",
        "        test_loss, test_accuracy = self.model.evaluate(test_data_generator)\n",
        "\n",
        "        # 保存模型和记录信息\n",
        "        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        model_filename = f\"model_{timestamp}.h5\"\n",
        "        self.model.save(os.path.join(self.model_save_path, model_filename))\n",
        "        self.record_info(model_filename, test_accuracy)\n",
        "\n",
        "        return f\"{model_filename}\", test_accuracy\n",
        "\n",
        "    def record_info(self, model_filename, test_accuracy):\n",
        "        with open(self.log_file_path, 'a') as log_file:\n",
        "            log_file.write(f\"Model: {model_filename}, Test Accuracy: {test_accuracy}, Date: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "\n",
        "# 使用示例\n",
        "model = # 加载您的模型\n",
        "trainer = ImageClassificationTrainer('/path/to/data', model, '/path/to/save/model', '/path/to/log.txt')\n",
        "test_data_generator = # 创建您的测试数据生成器\n",
        "model_name, test_accuracy = trainer.retrained_model(test_data_generator)"
      ],
      "metadata": {
        "id": "Wb9oc1tldQc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 步骤 2：在其他程序中导入"
      ],
      "metadata": {
        "id": "4cRQkto7d4Vx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from image_classification_trainer import ImageClassificationTrainer\n",
        "import tensorflow as tf\n",
        "\n",
        "# 假设您的模型定义如下\n",
        "model = tf.keras.models.load_model('/path/to/your/model.h5')\n",
        "\n",
        "# 创建训练器实例\n",
        "trainer = ImageClassificationTrainer('/path/to/data', model, '/path/to/save/model', '/path/to/log.txt')\n",
        "\n",
        "# 创建测试数据生成器\n",
        "test_data_generator = # 创建测试数据生成器\n",
        "\n",
        "# 使用训练器进行重新训练\n",
        "model_name, test_accuracy = trainer.retrained_model(test_data_generator)"
      ],
      "metadata": {
        "id": "LQkGw8tOeA3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. predict"
      ],
      "metadata": {
        "id": "9kzBbhtwSr2p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 判断给定路径的文件夹内是否包含其他子文件夹。"
      ],
      "metadata": {
        "id": "UTaLRVdsrdaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def predict_images_based_on_folder_structure(folder_path, model, target_size, class_labels):\n",
        "    \"\"\"\n",
        "    根据文件夹结构预测图片。\n",
        "\n",
        "    :param folder_path: 文件夹路径。\n",
        "    :param model: 预训练的模型。\n",
        "    :param target_size: 图像目标尺寸。\n",
        "    :param class_labels: 类别标签列表。\n",
        "    :return: 预测结果。\n",
        "    \"\"\"\n",
        "    # 检查是否存在子文件夹\n",
        "    contains_subfolders = any(os.path.isdir(os.path.join(folder_path, item)) for item in os.listdir(folder_path))\n",
        "\n",
        "    if contains_subfolders:\n",
        "        # 如果存在子文件夹，使用多文件夹预测函数\n",
        "        return predict_multiple_folders(folder_path, model, target_size, class_labels)\n",
        "    else:\n",
        "        # 否则，使用单文件夹预测函数\n",
        "        return predict_single_folder(folder_path, model, target_size, class_labels)\n"
      ],
      "metadata": {
        "id": "WMvJQuuerauQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 單個文件夾"
      ],
      "metadata": {
        "id": "duJKzDH1jRUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_single_folder(folder_path, model_path):\n",
        "    model = load_model(model_path)\n",
        "\n",
        "    predictions = {}\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.lower().endswith('.png'):\n",
        "            img_path = os.path.join(folder_path, filename)\n",
        "            img = load_and_preprocess_image(img_path)\n",
        "            pred = model.predict(np.expand_dims(img, axis=0))\n",
        "            predicted_class = class_labels[np.argmax(pred)]\n",
        "            predictions[filename] = {\n",
        "                'class': predicted_class,\n",
        "                'probabilities': pred[0].tolist()\n",
        "            }\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "0LNQpjfDjYZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 一個文件夾下有多個文件夾"
      ],
      "metadata": {
        "id": "yyYZE0YHjVIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_multiple_folders(parent_folder_path, model_path):\n",
        "    model = load_model(model_path)\n",
        "    all_predictions = {}\n",
        "    for folder_name in os.listdir(parent_folder_path):\n",
        "        folder_path = os.path.join(parent_folder_path, folder_name)\n",
        "        if os.path.isdir(folder_path):\n",
        "            predictions = predict_single_folder(folder_path, model_path)\n",
        "            all_predictions[folder_name] = predictions\n",
        "    return all_predictions"
      ],
      "metadata": {
        "id": "oL7AjfXjjYGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 預測"
      ],
      "metadata": {
        "id": "lZjW1oDffVz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "\n",
        "def predict_images(model_version, image_folder_path, target_size, class_labels):\n",
        "    \"\"\"\n",
        "    使用特定版本的模型预测图片文件夹中的所有图片。\n",
        "\n",
        "    :param model_version: 模型版本（模型文件名）。\n",
        "    :param image_folder_path: 图片文件夹路径。\n",
        "    :param target_size: 图像目标尺寸。\n",
        "    :param class_labels: 类别标签列表。\n",
        "    :return: 预测结果字典。\n",
        "    \"\"\"\n",
        "    # 加载模型\n",
        "    model = tf.keras.models.load_model(os.path.join('/path/to/saved/models', model_version))\n",
        "\n",
        "    # 预测结果字典\n",
        "    predictions = {}\n",
        "\n",
        "    # 遍历文件夹中的图片\n",
        "    for filename in os.listdir(image_folder_path):\n",
        "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "            img_path = os.path.join(image_folder_path, filename)\n",
        "            img = load_img(img_path, target_size=target_size)\n",
        "            img_array = img_to_array(img) / 255.0\n",
        "            img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "            # 进行预测\n",
        "            pred = model.predict(img_array)\n",
        "            class_index = np.argmax(pred, axis=1)[0]\n",
        "            class_name = class_labels[class_index]\n",
        "            probabilities = pred[0].tolist()\n",
        "\n",
        "            # 保存预测结果\n",
        "            predictions[filename] = {'class': class_name, 'probabilities': probabilities}\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# 使用示例\n",
        "model_version = 'model_20210908_1530.h5'  # 模型版本\n",
        "image_folder_path = '/path/to/image/folder'  # 图片文件夹路径\n",
        "target_size = (224, 224)  # 目标尺寸\n",
        "class_labels = ['class1', 'class2', 'class3', ...]  # 类别标签\n",
        "\n",
        "# 进行预测\n",
        "predictions = predict_images(model_version, image_folder_path, target_size, class_labels)\n",
        "\n",
        "# 打印或处理预测结果\n",
        "for filename, pred in predictions.items():\n",
        "    print(f\"{filename}: Class = {pred['class']}, Probabilities = {pred['probabilities']}\")\n"
      ],
      "metadata": {
        "id": "86pmiRLwfX8C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}